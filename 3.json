var x=[
 {
  "Name": "kushal",
  "Bookid": "B010",
  "Bookauthor": "schaller",
  "publicationyear": 1996,
  "genre": ["phisophy","mystery"],
  "noofcopies": 3,
  "review": "excellent"
 },
 {
  "Name": "kumar",
  "Bookid": "B011",
  "Bookauthor": "Barry",
  "publicationyear": 2000,
  "genre": ["fantasy"],
  "noofcopies": 8,
  "review": "good"
 },
 {
  "Name": "raj",
  "Bookid": "B012",
  "Bookauthor": "pranav",
  "publicationyear": 2001,
  "genre": ["thriller"],
  "noofcopies": 6,
  "review": "average"
 },
 {
  "Name": "rajesh",
  "Bookid": "B013",
  "Bookauthor": "helter",
  "publicationyear": 2010,
  "genre": ["horror","mystery"],
  "noofcopies": 2,
  "review": "average"
 },
 {
  "Name": "ram",
  "Bookid": "B014",
  "Bookauthor": "yardhi",
  "publicationyear": 2002,
  "genre": ["thriller","mystery"],
  "noofcopies": 1,
  "review": "bad"
 },
 {
  "Name": "peter",
  "Bookid": "B015",
  "Bookauthor": "john",
  "publicationyear": 1999,
  "genre": ["romantic","mystery"],
  "noofcopies": 9,
  "review": "average"
 },
 {
  "Name": "john",
  "Bookid": "B016",
  "Bookauthor": "raheme",
  "publicationyear": 2000,
  "genre": ["mystery"],
  "noofcopies": 2,
  "review": "good"
 }
]
db.books.insertMany(x);




























package Abhishek;
import java.io.IOException;
import java.util.Iterator;
import java.util.StringTokenizer;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;
public class wordCount {
public static class Map extends MapReduceBase implements 
Mapper<LongWritable, Text, Text, IntWritable>{
private Text word = new Text ();
@Override
public void map(LongWritable key, Text value, 
OutputCollector<Text, IntWritable> out, Reporter 
arg3)
throws IOException {
// TODO Auto-generated method stub
int one = 1 ;
String s1 = value.toString();
StringTokenizer token1 = new StringTokenizer(s1);
while(token1.hasMoreTokens()) {
word.set(token1.nextToken());
out.collect(word, new IntWritable(one));
}
}
}
public static class Reduce extends MapReduceBase implements 
Reducer<Text, IntWritable, Text, IntWritable>{
@Override
public void reduce(Text key, Iterator<IntWritable> value, 
OutputCollector<Text, IntWritable> out,
Reporter arg3) throws IOException {
// TODO Auto-generated method stub
int sum = 0 ;
while(value.hasNext()) {
sum += value.next().get();
}
out.collect(key, new IntWritable(sum));
}
}
public static void main(String[] args) throws IOException {
// TODO Auto-generated method stub
JobConf conf = new JobConf(wordCount.class);
conf.setJobName("ABC");
//final command :- hadoop jar jarname hdfsinputPath 
HDFSoutputPath
conf.setOutputKeyClass(Text.class);
conf.setOutputValueClass(IntWritable.class);
conf.setMapperClass(Map.class);
conf.setCombinerClass(Reduce.class);
conf.setReducerClass(Reduce.class);
conf.setInputFormat(TextInputFormat.class);
conf.setOutputFormat(TextOutputFormat.class);
FileInputFormat.setInputPaths(conf, new Path(args[0]));
FileOutputFormat.setOutputPath(conf, new Path(args[1]));
JobClient.runJob(conf);
}
}








sudo su -hadoop
cd HADOOP_HOME/sbin
./start-all.sh
jsp
cd ~
vim in.txt
hdfs dfs -put ~/in.txt ~/input/
hdfs dfs -ls ~/input
hadoop jar /home/jhgj.jar ~/input ~/outputt
hdfs dfs -ls ~/outputt
hdfs dfs -cat ~/output/part*
